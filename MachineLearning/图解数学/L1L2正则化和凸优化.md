## p范数
![](img/p_normal.jpg)
当 p>=1时，向量的L<sub>p</sub>范数是凸的。(这也是为什么一般不用L0范数的原因之一)

## 正则化
正则化项对b的更新没有影响，但是对于w的更新有影响。
L2正则（Ridge岭回归）和L1正则（Lasso回归）

![](img/normal_01.jpg)

> 因为范数影响了搜索空间（所以避免了w取所有值，就能一定程度上防止过拟合）。范数越小，说明w (也有叫β的)中越多项趋向于0，整个模型就越简单

L2转换为优化问题
![](img/normal_05.jpg)

L1转换为优化问题
![](img/normal_06.jpg)

图解L2和L1正则最优化
![](img/normal_02.jpg)

以二维情况讨论，上图左边是 L2 正则化，右边是 L1 正则化。从另一个方面来看，满足正则化条件，实际上是求解蓝色区域与黄色区域的交点，即同时满足限定条件和 Ein 最小化。对于 L2 来说，限定区域是圆，这样，得到的解 w1 或 w2 为 0 的概率很小，很大概率是非零的。


对于 L1 来说，限定区域是正方形，方形与蓝色区域相交的交点是顶点的概率很大，这从视觉和常识上来看是很容易理解的。也就是说，方形的凸点会更接近 Ein 最优解对应的 wlin 位置，而凸点处必有 w1 或 w2 为 0。这样，得到的解 w1 或 w2 为零的概率就很大了。所以，L1 正则化的解具有稀疏性。

扩展到高维，同样的道理，L2 的限定区域是平滑的，与中心点等距；而 L1 的限定区域是包含凸点的，尖锐的。这些凸点更接近 Ein 的最优解位置，而在这些凸点上，很多 wj 为 0。

为什么L1是方的，是有四条直线组成的，x1+x2=a 等四条直线组成
![](img/normal_03.jpg)

图解
![](img/normal_04.jpg)

### 什么时候用L1
只有非常少数的特征是重要的，可以用L1
需要稀疏性就用L1

> L2正则有解析解，而L1正则没有解析解，所以时间复杂度上L2要优于L1.
> 解析解,是指通过严格的公式所求得的解
> 在交叉点除不可求导，所以在该模型上其只能使用坐标轴下降法来逼近最优值，其在速率上远远慢于L2正则项，其次还可能无法达到很好的效果，因此在使用正则项的时候首选L2正则。

## 正则化项为什么能够防止过拟合
避免过拟合的方法有很多：early stopping、数据集扩增（Data augmentation）、正则化（Regularization）包括L1、L2（L2 regularization也叫weight decay权重衰减），dropout。

[机器学习：正则化项为什么能够防止过拟合？防止过拟合的方法](https://blog.csdn.net/qq_40765537/article/details/105320462)