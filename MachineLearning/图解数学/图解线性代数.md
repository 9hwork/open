## Generalized Linear Regression 通用线性回归

[Generalized Linear Regression](https://scikit-learn.org/stable/modules/linear_model.html#generalized-linear-regression)

[Generalized Linear Models Explained with Examples](https://vitalflux.com/generalized-linear-models-explained-with-examples/)

[Generalized Linear Model Theory - 推荐](https://data.princeton.edu/wws509/notes/a2.pdf)

[Generalized Linear Models](https://www.stat.cmu.edu/~ryantibs/advmethods/notes/glm.pdf)

> 这一家族中的模型形式基本上都差不多，不同的就是因变量(Y)不同，如果是连续的，就是多重线性回归，如果是二项分布，就是logistic回归，如果是poisson分布，就是poisson回归，如果是负二项分布，就是负二项回归，等等。只要注意区分它们的因变量就可以了。logistic回归的因变量可以是二分类的(二项逻辑回归)，也可以是多分类的（多项逻辑回归或者softmax回归），但是二分类的更为常用，也更加容易解释。所以实际中最为常用的就是二分类的logistic回归。

### 逻辑回归(也是一种广义线性模型)
[逻辑回归（非常详细）](https://zhuanlan.zhihu.com/p/74874291)


[6.2 Logistic Regression and the Cross Entropy Cost - Logistic regression sklearn中代价函数](https://jermwatt.github.io/machine_learning_refined/notes/6_Linear_twoclass_classification/6_3_Softmax.html)

> y属于0或1 (我们假设样本是服从伯努利分布(0-1分布)的，然后求得满足该分布的似然函数，最终求该似然函数的极大值)

[6.3 Logistic Regression and the Softmax Cost-Logistic regression sklearn中代价函数](https://jermwatt.github.io/machine_learning_refined/notes/6_Linear_twoclass_classification/6_3_Softmax.html)

> [y属于-1或1](https://github.com/jermwatt/machine_learning_refined/blob/gh-pages/notes/6_Linear_twoclass_classification/6_3_Softmax.ipynb)

> Logistic回归的损失函数:负对数似然,negative log likelihood(NLL), 负的log似然

> Softmax回归是Logistic回归的多分类情况。
> LogisticRegression 就是一个被logistic方程归一化后的线性回归。将预测的输出映射到0,1之间。

> 逻辑斯蒂回归模型的思想跟线性回归模型思想不一样，线性回归模型思想是最小化真实值与模型预测值的误差，而逻辑斯蒂回归模型思想就比较狠了，预测值预测对了损失函数就是0，错了损失就是无穷大，我个人的理解(一般采用的是-log(h(x)) 这是一个凸函数,刚好满足要求)

## 图解机器学习的数学直觉：线性代数，微积分，PCA
https://www.bilibili.com/video/BV1iW411T781


## 如何理解主元分析（PCA）？
https://matongxue.blog.csdn.net/article/details/82254488
[PCA的数学原理](http://blog.codinglabs.org/articles/pca-tutorial.html)

[通俗易懂的PCA原理及代码实现(超详细推导)](https://blog.csdn.net/MoreAction_/article/details/107463336)

## SVM 支持向量机算法-原理篇
https://www.cnblogs.com/codeshell/p/14301569.html

[胡浩基教授SVM](files/胡浩基-机器学习笔记/1. 支持向量机的理论推导.pdf)

[支持向量机-完整版](files/支持向量机.pdf)

[李宏毅深度学习笔记-SVM支持向量机](https://www.cnblogs.com/wry789/p/13110305.html)
深度学习和SVM的区别:
深度学习的隐藏层可以看成是特征转换，输出层可以看成是线性分类器。

SVM也是做类似的事情，先用一个核函数，把特征转换到高维空间上，再在高维空间使用线性分类器，线性分类器的损失函数一般使用hinge loss。


## 奇异值分解SVD
[如何通俗地理解奇异值](https://www.matongxue.com/madocs/306)
[奇异值的物理意义是什么？](https://www.zhihu.com/question/22237507/answer/225371236)

## 降维
[白板推导系列(系列五) 降维1-背景 视频](https://www.bilibili.com/video/BV1aE411o7qd?p=22)

13分的时候就知道为什么在边缘了

[白板推导系列(系列五) 降维1-背景 文字](https://www.yuque.com/books/share/f4031f65-70c1-4909-ba01-c47c31398466/kg2npf)

## 线性判别分析LDA

[线性判别分析LDA原理总结](https://www.cnblogs.com/pinard/p/6244265.html)
[LDA](http://www.sci.utah.edu/~shireen/pdfs/tutorials/Elhabian_LDA09.pdf)

### 高斯视角
高斯判别分析（Gaussian discriminant analysismodel, GDA）- 生成学习算法

[GDA（高斯判别分析）是 LDA（线性判别分析）和 QDA（二次判别分析）的通用术语，其中给定类别的每个观察的似然概率，即 P(x|y) 可以通过多变量建模高斯分布。](https://stats.stackexchange.com/questions/17614/gda-and-lda-terminology/269702#269702)

[LDA - QDA](http://personal.psu.edu/jol2/course/stat597e/notes2/lda.pdf)

[Linear and Quadratic Discriminant Analysis](https://scikit-learn.org/stable/modules/lda_qda.html)

[机器学习实现与分析之五（高斯判别分析）](http://blog.sina.com.cn/s/blog_13ec1876a0102xb48.html)

[【机器学习】高斯判别分析](https://zhuanlan.zhihu.com/p/95956492)

## 综合
[难懂的数学】傅里叶、拉普拉斯、卷积、欧拉方程、梯度散度、拉格朗日方程、奈奎斯特采样、虚数等抽象难懂数学一网打尽](https://www.bilibili.com/video/BV1kX4y1u7GJ)

[机器学习算法的动画可视化](https://davpinto.github.io/ml-simulations)

### 保序回归：单调不减
![Isotonic Regression](img/pava.gif) 

### K-means聚类
![](img/kmeans_01.gif)
1. 给定一组样本，确定k值，即类别数目
1. 随机初始化k个聚类的中心（质心，下称中心）
1. 计算每个样本与每一个中心的相似的程度——距离
1. 考察每个样本，离哪一个中心更近，就将该样本分为哪一类
1. 所有样本划分完毕后，根据已分好的数据，重新计算聚类的中心
1. 重复3、4、5，直到中心不变或者变化很小

[可视化k-means](https://www.naftaliharris.com/blog/visualizing-k-means-clustering/)
[可视化Visualizing DBSCAN Clustering](https://www.naftaliharris.com/blog/visualizing-dbscan-clustering/)

### Gaussian mixture models
[Gaussian mixture models](https://mbernste.github.io/posts/gmm_em/)
[极大似然估计、EM算法及高斯混合模型](https://blog.csdn.net/chris_xy/article/details/88970322)
[EM算法与GMM（高斯混合聚类）Jensen不等式和变分法两种推导](https://zhuanlan.zhihu.com/p/50686800)


### Latent Dirichlet Allocation
[用scikit-learn学习LDA主题模型](https://www.cnblogs.com/pinard/p/6908150.html)
[2.5.7. Latent Dirichlet Allocation (LDA)](https://scikit-learn.org/stable/modules/decomposition.html#latent-dirichlet-allocation-lda)
[文本主题模型之LDA(三) LDA求解之变分推断EM算法](https://www.cnblogs.com/pinard/p/6873703.html)

scikit-learn除了标准的变分推断EM算法外，还实现了另一种在线变分推断EM算法，它在原理篇里的变分推断EM算法的基础上，为了避免文档内容太多太大而超过内存大小，而提供了分步训练(partial_fit函数)，即一次训练一小批样本文档，逐步更新模型，最终得到所有文档LDA模型的方法。这个改进算法我们没有讲，具体论文在这
[Online Learning for Latent Dirichlet Allocation](https://papers.nips.cc/paper/2010/file/71f6278d140af599e06ad9bf1ba03cb0-Paper.pdf) M. Hoffman, D. Blei, F. Bach, 2010